{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NH_Inference.ipynb","provenance":[],"collapsed_sections":["Jhec0Lva1g_r","bZig34mA1zTY","0ebM0VXs18JI","16uECj4tXYJD","R-JI-B38Gx_p","C9s93sNI2fZa","-9dAhlyIdr5u","ODGy8FKqUUok","8h7QZDrsz2jJ","Z6fzLIcdUS3A","MRj5AEKm2k3s"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"68IXuoPZ3fQQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609704422870,"user_tz":-540,"elapsed":1658,"user":{"displayName":"신윤종","photoUrl":"","userId":"04808118463342931153"}},"outputId":"8c96a8a9-6ebf-4e02-9dc5-cf38de83dae6"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dcXcSVI73uOW"},"source":["%%capture \r\n","\r\n","!pip install transformers\r\n","!pip install pytorch_pretrained_bert==0.4.0\r\n","!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\r\n","%cd Mecab-ko-for-Google-Colab\r\n","! bash install_mecab-ko_on_colab190912.sh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jhec0Lva1g_r"},"source":["## 데이터 불러오기\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"nEPFA9ZQ5Ex8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609704468637,"user_tz":-540,"elapsed":47415,"user":{"displayName":"신윤종","photoUrl":"","userId":"04808118463342931153"}},"outputId":"5afb98b7-5ee6-4637-a87d-c39572a352a1"},"source":["%cd /content/drive/MyDrive/마이야르/6.Code \r\n","import config\r\n","import pandas as pd\r\n","\r\n","train = pd.read_csv(config.TRAIN_DIR)\r\n","test = pd.read_csv(config.TEST_DIR)\r\n","submission = pd.read_csv(config.SUBMISSION_DIR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1enR-TKNggkQ_bAdN7UF7isfRkeewUc_-/NH_dacon/예선/6.Code\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bZig34mA1zTY"},"source":["## Library 불러오기"]},{"cell_type":"code","metadata":{"id":"Ydc04zKHJefz"},"source":["import time\r\n","start = time.time()\r\n","\r\n","import os\r\n","import sys\r\n","import random\r\n","import easydict \r\n","from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\r\n","from pytorch_pretrained_bert.modeling import BertForSequenceClassification as ETRI_BertForSequenceClassification\r\n","from pytorch_pretrained_bert.modeling import BertConfig, WEIGHTS_NAME, CONFIG_NAME\r\n","\r\n","import re\r\n","import datetime\r\n","import warnings\r\n","from collections import Counter, OrderedDict\r\n","\r\n","import numpy as np\r\n","from konlpy.tag import Mecab\r\n","from lightgbm import LGBMClassifier \r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","import torch\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","from transformers import BertTokenizer, BertForSequenceClassification\r\n","\r\n","import module\r\n","import preprocess\r\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n","warnings.filterwarnings(action='ignore')\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ebM0VXs18JI"},"source":["## pos_Tagger, Tokenizer, pretrained_embedding, Model 불러오기"]},{"cell_type":"code","metadata":{"id":"uXV_edjH2BXl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609704493480,"user_tz":-540,"elapsed":72249,"user":{"displayName":"신윤종","photoUrl":"","userId":"04808118463342931153"}},"outputId":"800db62d-92db-4027-e4b1-86b8eedd1437"},"source":["'''\r\n","Tokenizer\r\n","'''\r\n","mecab = Mecab()\r\n","\r\n","'''\r\n","BERT Multilingual Loading\r\n","'''\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\r\n","tokenizer.bos_token = tokenizer.cls_token\r\n","tokenizer.eos_token = tokenizer.sep_token\r\n","bert_ml_model = BertForSequenceClassification.from_pretrained(config.BERT_ML_PATH, num_labels=2)\r\n","\r\n","\r\n","'''\r\n","ETRI Model Loading\r\n","'''\r\n","vocab2id = preprocess.construct_vocab(config.VOCAB2ID_PATH)\r\n","args = easydict.EasyDict({\"output_dir\":config.ETRI_OUTPUT_DIR,\r\n","                          \"max_seq_length\":200,\r\n","                          \"do_lower_case\":False,\r\n","                          \"eval_batch_size\":32,\r\n","                          \"no_cuda\": False,\r\n","                          \"local_rank\":-1,\r\n","                          \"seed\":42})\r\n","test2 = test.copy()\r\n","train2  = train.copy()\r\n","label_list = [\"0\", \"1\"]\r\n","num_labels = num_labels_task = len(label_list)\r\n","output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\r\n","output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\r\n","\r\n","bert_config = BertConfig(output_config_file)\r\n","bert_etri_model = ETRI_BertForSequenceClassification(bert_config, num_labels=num_labels)\r\n","bert_etri_model.load_state_dict(torch.load(output_model_file))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"markdown","metadata":{"id":"16uECj4tXYJD"},"source":["## 형태소 분석 + 전처리"]},{"cell_type":"code","metadata":{"id":"hCab2_pHXc96"},"source":["'''\r\n","특수 문자 전처리\r\n","'''\r\n","train[\"title\"] = train.apply(lambda x : preprocess.clean_text(x[\"title\"]), axis=1)\r\n","train[\"content\"] = train.apply(lambda x : preprocess.clean_text(x[\"content\"]), axis=1)\r\n","\r\n","test[\"title\"] = test.apply(lambda x : preprocess.clean_text(x[\"title\"]), axis=1)\r\n","test[\"content\"] = test.apply(lambda x : preprocess.clean_text(x[\"content\"]), axis=1)\r\n","\r\n","'''\r\n","토크나이징\r\n","'''\r\n","train[\"title_tokenized\"] = train[\"title\"].apply(lambda x : preprocess.get_pos(x, mecab))\r\n","train[\"content_tokenized\"] = train[\"content\"].apply(lambda x : preprocess.get_pos(x, mecab))\r\n","\r\n","test[\"title_tokenized\"] = test[\"title\"].apply(lambda x : preprocess.get_pos(x, mecab))\r\n","test[\"content_tokenized\"] = test[\"content\"].apply(lambda x : preprocess.get_pos(x, mecab))\r\n","\r\n","'''\r\n","BAD tokens\r\n","'''\r\n","train[\"title_morphs\"] = train['title_tokenized'].apply(lambda x : [token.split('/')[0] for token in x])\r\n","train[\"content_morphs\"] = train['content_tokenized'].apply(lambda x : [token.split('/')[0] for token in x])\r\n","\r\n","test[\"title_morphs\"] = test['title_tokenized'].apply(lambda x : [token.split('/')[0] for token in x])\r\n","test[\"content_morphs\"] = test['content_tokenized'].apply(lambda x : [token.split('/')[0] for token in x])\r\n","\r\n","fake_tokens = train[train[\"info\"] == 1][\"content_morphs\"]\r\n","fake_tokens = [a for b in fake_tokens for a in b]\r\n","fake_freq = Counter(fake_tokens)\r\n","\r\n","real_tokens = train[train[\"info\"] == 0][\"content_morphs\"]\r\n","real_tokens = [a for b in real_tokens for a in b]\r\n","real_freq = Counter(real_tokens)\r\n","\r\n","bad_tokens = {}\r\n","for token, fake in fake_freq.items():\r\n","    if token in real_freq.keys():\r\n","        real = real_freq[token]\r\n","    else:\r\n","        real = 0\r\n","        \r\n","    if fake > real * 5 and fake > 20 and len(token) > 1:\r\n","        bad_tokens[token] = fake\r\n","\r\n","'''\r\n","중복셋 전처리\r\n","'''\r\n","train_unq = train.drop_duplicates(subset='content')  # 학습셋 유니크\r\n","test_dup = test[test['content'].isin(train_unq['content'].values)] # 테스트셋과 중복 찾기\r\n","test_dup['idx'] = test_dup.index  # 원본 테스트셋 인덱스 보존\r\n","test_duplicated = pd.merge(test_dup, train_unq[['content', 'info']], on='content', how='left')  # 5만개 중복셋 고정 라벨 \r\n","test_unq = test[~test.index.isin(test_duplicated['idx'].values)]\r\n","test_idx = test_unq.index.values\r\n","\r\n","train_unq2 = train2.drop_duplicates(subset='content')  \r\n","test_dup2 = test2[test2['content'].isin(train_unq2['content'].values)] \r\n","test_dup2['idx'] = test_dup2.index  \r\n","test_duplicated2 = pd.merge(test_dup2, train_unq2[['content', 'info']], on='content', how='left') \r\n","test_duplicated2 = test_duplicated2[test_duplicated2['info'] == 1]\r\n","test_unq2 = test2[~test2.index.isin(test_duplicated2['idx'].values)]\r\n","test_idx2 = test_unq2.index.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-JI-B38Gx_p"},"source":["## Feature Engineering"]},{"cell_type":"code","metadata":{"id":"C5B_RuOZ0pq5"},"source":["# 하나의 타이틀에 대한 기사 개수 \r\n","total_train = pd.DataFrame(train.groupby([\"n_id\", \"title\"]).size()).reset_index().rename(columns={0 : 'total'})\r\n","train = pd.merge(train, total_train[[\"n_id\", \"total\"]], on = 'n_id', how='left')    # 'total' feature 붙인 것 \r\n","total_test = pd.DataFrame(test.groupby([\"n_id\", \"title\"]).size()).reset_index().rename(columns={0 : 'total'})\r\n","test = pd.merge(test, total_test[[\"n_id\", \"total\"]], on = 'n_id', how='left')       # 'total' feature 붙인 것 \r\n","\r\n","# 제목에 특정 단어 포함\r\n","train[\"word1\"] = train.apply(lambda x : x[\"title\"].count('52주 신고가') + x[\"content\"].count('52주 신고가'), axis=1)\r\n","train[\"word2\"] = train[\"title\"].apply(lambda x : 1 if x in ('급등 중') else 0)\r\n","train[\"word3\"] = train[\"title\"].apply(lambda x : 1 if re.search(r'\\(\\d{6}\\)', x) else 0)\r\n","train[\"word4\"] = train[\"title\"].apply(lambda x : 1 if x in ('상승|하락') else 0)\r\n","train[\"word5\"] = train.apply(lambda x : x[\"title\"].count('급반등') + x[\"content\"].count('급반등'), axis=1)\r\n","train[\"word6\"] = train.apply(lambda x : x[\"title\"].count('코로나') + x[\"content\"].count('코로나'), axis=1)\r\n","train[\"word7\"] = train.apply(lambda x : x[\"title\"].count('유망') + x[\"content\"].count('유망'), axis=1)\r\n","train[\"word8\"] = train[\"title\"].apply(lambda x : 1 if x in ('클릭|Click') else 0)\r\n","test[\"word1\"] = test.apply(lambda x : x[\"title\"].count('52주 신고가') + x[\"content\"].count('52주 신고가'), axis=1)\r\n","test[\"word2\"] = test[\"title\"].apply(lambda x : 1 if x in ('급등 중') else 0)\r\n","test[\"word3\"] = test[\"title\"].apply(lambda x : 1 if re.search(r'\\(\\d{6}\\)', x) else 0)\r\n","test[\"word4\"] = test[\"title\"].apply(lambda x : 1 if x in ('상승|하락') else 0)\r\n","test[\"word5\"] = test.apply(lambda x : x[\"title\"].count('급반등') + x[\"content\"].count('급반등'), axis=1)\r\n","test[\"word6\"] = test.apply(lambda x : x[\"title\"].count('코로나') + x[\"content\"].count('코로나'), axis=1)\r\n","test[\"word7\"] = test.apply(lambda x : x[\"title\"].count('유망') + x[\"content\"].count('유망'), axis=1)\r\n","test[\"word8\"] = test[\"title\"].apply(lambda x : 1 if x in ('클릭|Click') else 0)\r\n","\r\n","# POS tagger morphs\r\n","train[\"title_tkn_wo_pos\"] = train['title_tokenized'].apply(lambda x : [token.split('/')[0] for token in x])\r\n","test[\"title_tkn_wo_pos\"] = test['title_tokenized'].apply(lambda x : [token.split('/')[0] for token in x])\r\n","train[\"content_tkn_wo_pos\"] = train['content_tokenized'].apply(lambda x : [token.split('/')[0] for token in x])\r\n","test[\"content_tkn_wo_pos\"] = test['content_tokenized'].apply(lambda x : [token.split('/')[0] for token in x])\r\n","\r\n","# BAD tokens: 가짜뉴스에 많은 단어 \r\n","fake_tokens = train[train[\"info\"] == 1][\"content_tkn_wo_pos\"]\r\n","f_tokens = [a for b in fake_tokens for a in b]\r\n","f_tokens = Counter(f_tokens)\r\n","fake_freq = pd.Series(f_tokens).sort_values(ascending=False)\r\n","real_tokens = train[train[\"info\"] == 0][\"content_tkn_wo_pos\"]\r\n","r_tokens = [a for b in real_tokens for a in b]\r\n","r_tokens = Counter(r_tokens)\r\n","real_freq = pd.Series(r_tokens).sort_values(ascending=False)\r\n","\r\n","bad_tokens = {}\r\n","for token, fake in zip(fake_freq.index, fake_freq):\r\n","    if token in real_freq.index:\r\n","        real = real_freq.loc[token]\r\n","    else:\r\n","        real = 0\r\n","        \r\n","    if fake > real * 5 and fake > 20 and len(token) > 1:\r\n","        bad_tokens[token] = fake\r\n","        \r\n","train[\"title_bad\"] = train[\"title_tkn_wo_pos\"].apply(lambda x : preprocess.count_badtokens(x, bad_tokens))\r\n","test[\"title_bad\"] = train[\"title_tkn_wo_pos\"].apply(lambda x : preprocess.count_badtokens(x, bad_tokens))\r\n","train[\"content_bad\"] = train[\"content_tkn_wo_pos\"].apply(lambda x : preprocess.count_badtokens(x, bad_tokens))\r\n","test[\"content_bad\"] = train[\"content_tkn_wo_pos\"].apply(lambda x : preprocess.count_badtokens(x, bad_tokens))\r\n","\r\n","# Content 길이\r\n","train[\"content_length\"] = train.apply(lambda x : len(x[\"content\"]), axis=1)\r\n","train[\"content_words\"] = train.apply(lambda x : len(x[\"content\"].split()), axis=1)\r\n","train[\"content_tokens\"] = train.apply(lambda x : len(x[\"content_tokenized\"]), axis=1)\r\n","test[\"content_length\"] = test.apply(lambda x : len(x[\"content\"]), axis=1)\r\n","test[\"content_words\"] = test.apply(lambda x : len(x[\"content\"].split()), axis=1)\r\n","test[\"content_tokens\"] = test.apply(lambda x : len(x[\"content_tokenized\"]), axis=1)\r\n","\r\n","# 괄호\r\n","train[\"bracket1\"] = train[\"title\"].apply(lambda x : 1 if x in ('\\(') else 0)\r\n","train[\"bracket2\"] = train[\"title\"].apply(lambda x : 1 if x in ('\\(|\\<|\\【') else 0)\r\n","test[\"bracket1\"] = test[\"title\"].apply(lambda x : 1 if x in ('\\(') else 0)\r\n","test[\"bracket2\"] = test[\"title\"].apply(lambda x : 1 if x in ('\\(|\\<|\\【') else 0)\r\n","\r\n","# 기사개수\r\n","train[\"total1\"] = train[\"total\"].apply(lambda x : 1 if x==1 else 0)\r\n","test[\"total1\"] = test[\"total\"].apply(lambda x : 1 if x==1 else 0)\r\n","train[\"total5\"] = train[\"total\"].apply(lambda x : 1 if 1<x<=5 else 0)\r\n","test[\"total5\"] = test[\"total\"].apply(lambda x : 1 if 1<x<=5 else 0)\r\n","\r\n","# Ord\r\n","train[\"ord18\"] = train[\"ord\"].apply(lambda x : 1 if x <= 18 else 0)\r\n","test[\"ord18\"] = test[\"ord\"].apply(lambda x : 1 if x <= 18 else 0)\r\n","\r\n","# 날짜 관련 통계량\r\n","count = pd.DataFrame(train.groupby([\"n_id\", \"title\", \"info\"]).size()).reset_index().rename(columns={0 : 'count'})     \r\n","fake = count[count[\"info\"] == 1].rename(columns={'count' : 'fake'})\r\n","real = count[count[\"info\"] == 0].rename(columns={'count' : 'real'}) \r\n","train = pd.merge(train, fake[[\"n_id\", \"fake\"]], on = 'n_id', how='left')    # fake \r\n","train = pd.merge(train, real[[\"n_id\", \"real\"]], on = 'n_id', how='left')    # real \r\n","\r\n","def ratio(a, b) : \r\n","    return a / b\r\n","train[\"fake_ratio\"] = train.apply(lambda x : ratio(x[\"fake\"], x[\"total\"]), axis=1)  # 가짜 / 전체 \r\n","train[\"real_ratio\"] = train.apply(lambda x : ratio(x[\"real\"], x[\"total\"]), axis=1)  # 진짜 / 전체 \r\n","temp = train.groupby('date').agg({\r\n","    'fake_ratio' : [('fake_ratio_max', np.max),\r\n","                    ('fake_ratio_min', np.min),\r\n","                    ('fake_ratio_mean', np.mean),\r\n","                    ('fake_ratio_median', np.median)\r\n","                    ], \r\n","    'real_ratio' : [('real_ratio_max', np.max),\r\n","                    ('real_ratio_min', np.min),\r\n","                    ('real_ratio_mean', np.mean),\r\n","                    ('real_ratio_median', np.median)\r\n","                    ],     \r\n","}).reset_index()\r\n","train = train.merge(temp, on = 'date', how = 'left')\r\n","test = test.merge(temp, on = 'date', how = 'left')\r\n","\r\n","# 자카드 유사도\r\n","train[\"jaccard_similarity\"] = train.apply(lambda x : preprocess.jaccard_sim(x[\"title_tkn_wo_pos\"], x[\"content_tkn_wo_pos\"]), axis = 1)\r\n","test[\"jaccard_similarity\"] = test.apply(lambda x : preprocess.jaccard_sim(x[\"title_tkn_wo_pos\"], x[\"content_tkn_wo_pos\"]), axis = 1)\r\n","\r\n","train_feature = train.drop(['date', 'title', 'content', 'real', 'fake', 'fake_ratio', 'real_ratio', 'title_tokenized', 'content_tokenized', \\\r\n","                            'title_morphs', 'content_morphs', 'title_processed', 'content_processed', 'title_tkn_wo_pos', 'content_tkn_wo_pos'], axis=1, errors='ignore')\r\n","test_feature = test.drop(['date', 'title', 'content', 'real', 'fake', 'fake_ratio', 'real_ratio', 'title_tokenized', 'content_tokenized', \\\r\n","                            'title_morphs', 'content_morphs', 'title_processed', 'content_processed', 'title_tkn_wo_pos', 'content_tkn_wo_pos'], axis=1, errors='ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C9s93sNI2fZa"},"source":["## 예측"]},{"cell_type":"markdown","metadata":{"id":"-9dAhlyIdr5u"},"source":["### LGBM"]},{"cell_type":"code","metadata":{"id":"CifL7PEa4yK5"},"source":["X = train_feature.drop([\"n_id\", \"info\"], axis=1)\r\n","y = train_feature[\"info\"]\r\n","X_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 2020, test_size=0.2)\r\n","\r\n","lgbm = LGBMClassifier(n_estimators=200)\r\n","lgbm.fit(X_train, y_train)\r\n","lgbm_preds = lgbm.predict(test_feature.drop([\"n_id\", \"id\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ODGy8FKqUUok"},"source":["### Bert Classifier - multilingual"]},{"cell_type":"code","metadata":{"id":"mckqNjHZoO_r"},"source":["'''\r\n","Dataloader \r\n","'''\r\n","MAX_LEN = 128\r\n","torch.cuda.empty_cache()\r\n","\r\n","content_processed = ['[CLS] ' + title + ' [SEP] ' + content + ' [SEP] ' for title, content in zip(test_unq['title'].values, test_unq['content'].values)]\r\n","\r\n","tokens = [tokenizer.tokenize(sent) for sent in content_processed]\r\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokens]\r\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=1)\r\n","attention_masks = [[float(i != 1) for i in ids] for ids in input_ids]\r\n","\r\n","inputs = torch.tensor(input_ids)\r\n","masks = torch.tensor(attention_masks)\r\n","\r\n","dataset = TensorDataset(inputs, masks)\r\n","dataloader = DataLoader(dataset, batch_size = 32)\r\n","\r\n","'''\r\n","Inference\r\n","'''\r\n","bert_ml_model = bert_ml_model.to(device)\r\n","bert_ml_model.eval()\r\n","\r\n","ml_preds = [] \r\n","for b_input_ids, b_input_mask in dataloader:\r\n","    b_input_ids = b_input_ids.to(device)\r\n","    b_input_mask = b_input_mask.to(device)\r\n","\r\n","    with torch.no_grad():     \r\n","        outputs = bert_ml_model(b_input_ids, \r\n","            token_type_ids=None, \r\n","            attention_mask=b_input_mask)\r\n","\r\n","    logits = outputs[0].detach().cpu().numpy()\r\n","    pred = np.argmax(logits, axis=1)  # list  \r\n","    ml_preds.extend(pred)\r\n","    \r\n","ml_pred = {i: x for i, x in zip(test_idx, ml_preds)}\r\n","ml_dup = {i: x for i, x in zip(test_duplicated['idx'].values, test_duplicated['info'].values)}\r\n","ml_pred.update(ml_dup)\r\n","\r\n","bert_ml_preds = list(OrderedDict(sorted(ml_pred.items())).values())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8h7QZDrsz2jJ"},"source":["### Bert Classifier - ETRI"]},{"cell_type":"code","metadata":{"id":"dDAIAgUN26NC"},"source":["'''\r\n","ETRI Model Setting\r\n","'''\r\n","torch.cuda.empty_cache()\r\n","random.seed(args.seed)\r\n","np.random.seed(args.seed)\r\n","torch.manual_seed(args.seed)\r\n","\r\n","processors = module.ColaProcessor(title_column='title_tokenized2', content_column='content_tokenized2')\r\n","    \r\n","if n_gpu > 0:\r\n","    torch.cuda.manual_seed_all(args.seed)\r\n","\r\n","'''\r\n","ETRI Model Inference\r\n","'''\r\n","test2 = pd.read_csv(config.TEST_DIR)\r\n","train2 = pd.read_csv(config.TRAIN_DIR)\r\n","train_unq2 = train2[train2['info'] == 1]\r\n","train_unq2 = train_unq2.drop_duplicates(subset='content')  \r\n","test_dup2 = test2[test2['content'].isin(train_unq2['content'].values)] \r\n","test_dup2['idx'] = test_dup2.index  \r\n","test_duplicated2 = pd.merge(test_dup2, train_unq2[['content', 'info']], on='content', how='left') \r\n","test_unq2 = test2[~test2.index.isin(test_duplicated2['idx'].values)]\r\n","test_idx2 = test_unq2.index.values\r\n","\r\n","test_unq2[\"title_tokenized2\"] = test_unq2[\"title\"].apply(lambda x : preprocess.get_pos(x, mecab))\r\n","test_unq2[\"content_tokenized2\"] = test_unq2[\"content\"].apply(lambda x : preprocess.get_pos(x, mecab))\r\n","\r\n","test_examples = processors.get_test_examples(test_unq2)\r\n","test_features = module.convert_examples_to_features(\r\n","    test_examples, label_list, args.max_seq_length, preprocess.get_pos, vocab2id)\r\n","\r\n","input_ids = torch.LongTensor([f.input_ids for f in test_features])\r\n","input_masks = torch.LongTensor([f.input_mask for f in test_features])\r\n","segment_ids = torch.LongTensor([f.segment_ids for f in test_features])\r\n","\r\n","eval_data = TensorDataset(input_ids, input_masks, segment_ids)\r\n","eval_sampler = SequentialSampler(eval_data)\r\n","eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\r\n","\r\n","bert_etri_model = bert_etri_model.to(device)\r\n","bert_etri_model.eval()\r\n","\r\n","etri_preds = []\r\n","for input_id, input_mask, segment_id in eval_dataloader:\r\n","    input_id = input_id.to(device)\r\n","    input_mask = input_mask.to(device)\r\n","    segment_id = segment_id.to(device)\r\n","\r\n","    with torch.no_grad():\r\n","        outputs = bert_etri_model(input_id, segment_id, input_mask)\r\n","\r\n","    logits = outputs.detach().cpu().numpy()\r\n","    pred = np.argmax(logits, axis=1)\r\n","    etri_preds.extend(pred)\r\n","    \r\n","etri_pred = {i: x for i, x in zip(test_idx2, etri_preds)}\r\n","etri_dup = {i: x for i, x in zip(test_duplicated2['idx'].values, test_duplicated2['info'].values)}\r\n","etri_pred.update(etri_dup)\r\n","\r\n","bert_etri_preds = list(OrderedDict(sorted(etri_pred.items())).values())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6fzLIcdUS3A"},"source":["### Ensemble"]},{"cell_type":"code","metadata":{"id":"rs2Vp02LcYJK"},"source":["ensemble = [] \r\n","\r\n","for a, b, c in zip(bert_etri_preds, bert_ml_preds, lgbm_preds): \r\n","    if a == b == c : \r\n","        ensemble.append(a)\r\n","    elif a != b : \r\n","        ensemble.append(c)\r\n","    elif (a == b and b != c) : \r\n","        ensemble.append(a)\r\n","\r\n","submission['info'] = ensemble"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRj5AEKm2k3s"},"source":["## 제출"]},{"cell_type":"code","metadata":{"id":"7rCCwNeRj_kT"},"source":["print(time.time() - start)\r\n","\r\n","submission.to_csv('/content/submission.csv')"],"execution_count":null,"outputs":[]}]}